---
title: "AbstractAnalysis"
author: "Kelsey Bezaire"
date: "2023-02-06"
output: 
  html_document:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Systematic Review Analysis
## Purpose

This code is intended to clean, prepare, and analyse systematic review data from the abstract and title screening stage. These steps may be broken up into separate files within the project for clarity.

## Data Analysis
From the published methods section:
Descriptive statistics of articles screened at the title and abstract level was used to identify gross trends in the literature and to explore methodology. Descriptive statistics included:
X publication trend of number of papers by date and by journal, 
X number of papers by first author location, if listed. 

Text mining was used to pull relevant words from abstracts to explore:
X frequency of word, 
- word frequency by year 
X with correlation tests 
Sentiment analysis was used to 
- separate words into sentiments, such as words preceded by 'not' belonging to negative sentiments, and 
- word count by sentiment. 

Analysis included: 
- visualising a network of words used together and a correlation of words including 
- a visualisation of correlations within word clusters. 

Text mining was conducted using tidytext package in R \citep{Silge2016}. From the list of relevant words, methodology-based words were isolated and counted for frequency. For example, words like behaviour observation, preference test, cortisol, oxytocin, and other such measures pertaining to affect or welfare.

The top 3 most-frequently mentioned method will be used as the selection criterion for full-text screening and appraisal, to not exceed 20\% (n=62) of all considered papers.

### Quick tips
Cmd + Option + I for a new chunk.

## Load Packages
```{r}
# install.packages('SnowballC')
install.packages("widyr")
```

```{r}
library(tidyverse)
library(Hmisc)
library(dplyr)
library(ggplot2)
library(ggridges) # for ridgeplot
library(viridis)
library(countrycode)
library(maps)
library(extrafont)

library(stringr)        # text cleaning and regular expressions
library(tidytext)       # provides additional text mining functions
library(wordcloud)
library(widyr)
library(SnowballC) # This is stemming in NLP

library(igraph) # For network graphs
library(ggraph)


```
```{r}
# Citation code for write-up. Delete me later.
citation("SnowballC")
```

###Plotting theme setting for consistency

```{r}
theme_set(theme_bw(base_size=12, base_family='sans')+
             theme(panel.grid.major = element_blank(), 
                   panel.grid.minor = element_blank()))
```


## Data Cleaning
### Load file

```{r}
rawdata <- read_csv("RayyanExport06022023/articles.csv")

```
```{r}
spec(rawdata)
```

```{r}
rawcustom <- read_csv("RayyanExport06022023/customizations_log.csv")
spec(rawcustom)
```

```{r}
rawdata
```

```{r}
rawcustom
```

So it looks like the customisation log contains every change to every article made. This can be helpful to use as a way to find what articles were included, but from looking at the rawdata, the inclusion decisions are listed in the notes section. However, the notes section contains a lot of other information. I don't want to list every detail out in its own column to tidy this; I just want to select those that have been included. 

**Solution**
So I'll filter for strings containing RAYYAN-INCLUSION: {"Kelsey"=>"Included" or pass new rule (does that apply to all included though?)

Not sure how to do this withe parenthesis and brackets involved. Maybe I can start by filtering for just 'included' and if that doesn't work, I can try to escape the special characters with \ as in \"Hello\"

You know what, let's try to separate them first so it's easier to deal with.

```{r}
# doesn't work like I want it to
rawdata %>% 
  separate(notes, into = c("misc", "rating", "exclusion"), sep = "|", convert=TRUE)
```
Let's try plan B for a moment and get back to that if we have to.

For the below, I first tried 'pass new rule' and only got 43 rows. Then I realised pass new rule was a label I made when testing something out (a new more strict criterion that we didn't include). When I filtered for 'Included', the number matches the number included in our study. Woo!
```{r}
rawdata %>%
  filter(str_detect(notes, "Included"))
```

Let's assign that as a new variable now called included.

```{r}
included <- rawdata %>%
  filter(str_detect(notes, "Included"))
```


Before doing anything more, let's look to see if there are important missing values.
```{r}
Hmisc::describe(included)
```

It looks like location is missing for a lot of papers, so that's going to need fixing before we can look at location data. The year is missing for one paper. We should isolate that and find out what year it is before moving on. All the papers have titles and abstracts and journals, and only 3 are missing keywords. This is good. Some are missing DOI and URL. This is something to note when I try to pull full articles later.

Let's first isolate the paper that is missing the year and take care of that right now.

```{r}
yearmia <- included %>%
  filter(is.na(year))
yearmia
```
I found the paper! It was published in 2021. I don't have a precedent for this. I'll discuss with my coauthors and put it in the amendment document. I think that articles with missing information important for analysis will have the information manually added directly from the published article. If the paper with the missing data can't be located, only then will it be dropped from analysis. I've noted this decision change in amendment no. 11.

Now to find the papers missing keywords. (I don't have specific keyword analysis but I might need this for hte word relationship bit. Better to have complete data than missing data.)
```{r}
# missing keywords
keymia <- included %>%
  filter(is.na(keywords))
keymia

# missing addresses
addressmia <- included %>%
  filter(is.na(location))
addressmia
```

I'll save them all in a subfolder called manually_corrected. Then we'll put revisions into these documents, save with corrected name, then recombine the files together. That way we're not changing the raw download file.
Files will be saved as 'data needs X' and once the corrections are in will be saved as 'data has X'.

```{r}
# save missing year file
write_csv(yearmia, path = "RayyanExport06022023/manually_corrected/data_needs_year.csv")
# save missing keywords
write_csv(keymia, path = "RayyanExport06022023/manually_corrected/data_needs_keywords.csv")
# save missing addresses
write_csv(addressmia, path = "RayyanExport06022023/manually_corrected/data_needs_address.csv")
```

NOTE: looking over the data that has a location, there are multiple countries listed, so I'll need to clean up that data before I can do location analysis on the first author location. I think if I slice up the string to only include everything before a . or ; or if neither are in a location cell, include the whole string. The . should catch most of it as I've noticed they syntax seems to be a stop after listing the country and before listing the next author's information.

It's really messy as sometimes names and emails and telephone numbers are listed all in the same cell with the location. I'll probably have to just isolate by country?

So let's load the files with corrected data and merge them. With the addresses, I opened them in Excel to make pasting data easier, but it changed the formatting on some content, so I deleted everything but the key and location columns. I did the same with the other files just in case opening the CSV in numbers changed any formatting. 

```{r}
# Load missing year file
withyear <- read_csv("RayyanExport06022023/manually_corrected/data_has_year.csv")
# Load missing keyword file
withkeyword <- read_csv("RayyanExport06022023/manually_corrected/data_has_keyword.csv")
# Load missing location file
withaddress <- read_csv("RayyanExport06022023/manually_corrected/data_has_address.csv")
```
To check that the merge is correct, let's see how many lines each dataset has. When we merge we'll want the length to be the same

```{r}
dim(included)
dim(withaddress)
dim(withkeyword)
dim(withyear)
```


I had some difficulty recombining the data based on key. I tried merge and group_by and left_join and bind_rows. In the end it is easier to combine the data by key, which means no new rows but creates new columns (it changes the column names to end in .x and .y), and then combine the data from both columns to a new column, and finally remove the old columns that had NAs. It is not the best, but it works and is easy to read.

```{r}
# use full join to combine data matching the keyword
testjoin1 <- full_join(included, withkeyword, by="key")
testjoin2 <- full_join(testjoin1, withaddress, by="key")
completedata <- full_join(testjoin2, withyear, by="key")

# create new columns that combine the data from the data.x and data.y columns that were created by doing the full_join
completedata$keywords <- coalesce(completedata$keywords.x, completedata$keywords.y)
completedata$year <- coalesce(completedata$year.x, completedata$year.y)
completedata$location <- coalesce(completedata$location.x, completedata$location.y)

# Now to remove the unnecessary data.x and data.y columns
tidierincluded <- completedata %>% select(-c(keywords.x, keywords.y, location.x, location.y, year.x, year.y))

# take a look
summary(tidierincluded)
```

## Publication by Year

Let's look at the year data.
```{r echo=FALSE}
ggplot(tidierincluded, aes(year)) + 
  geom_histogram(bins=20, color = "turquoise4", fill = "aquamarine3") +
  geom_vline(aes(xintercept = median(year)), color = "midnightblue", size = 1) +
  geom_vline(aes(xintercept = median(year) + sd(year)), color = "midnightblue", size = 0.5) +
  geom_vline(aes(xintercept = median(year) - sd(year)), color = "midnightblue", size = 0.5) +
  scale_x_continuous(breaks=seq(1960,2022,5)) +
  labs(
    title = "Article Publication by Year",
    subtitle = "From 1966 to May 2022",
    # caption = "Articles included after title and abstract screening. Vertical lines depict the median and standard deviation.",
    x = "Year of article publication",
    y = "Count of articles published",
    family="Kohinoor Devanagari"
  ) +
  theme_classic()

ggsave("PubbyYear.png", width = 5, height = 4)
```
```{r}
print("The median article year is:")
median(tidierincluded$year)
print("And standard deviation of article year is:")
sd(tidierincluded$year)

print("The earliest article year is:")
min(tidierincluded$year)
print("The most recent article year is:")
max (tidierincluded$year)
```

Now to look at the number of articles by journal.
I tried a density ridgeline and it looks bad. Tried a binned one and it looks better but still bad. Too many journals!

So some of the journals are not grouping well because of punctuation and case sensitivity. I've got over 90 articles but some are clearly the same like plos ONE and plos one. Let's tidy these up and try again. I'm down to 82 but there are still some of the same journals with different ways of presenting the name.

So the title content changes will be:
- wiener tierärztliche monatsschrift without the umlaut.
- journal of applied animal welfare science jaaws without the jaaws
- journal of the american veterinary medical association without the javma
- animals with just the short title
- anthrozoos as one word

With this change we now have 78 unique journals tht we can now analyse

```{r}
# Journal titles to lowercase
tidierincluded$journal <- tolower(tidierincluded$journal)

# remove punctuation
tidierincluded$journal <- gsub("[[:punct:][:blank:]]+", " ", tidierincluded$journal)

# Replace incidence of unwanted journal title with wanted journal title.

# detect unwanted journal name strings and replace them with wanted names
tidier2 <- tidierincluded %>%
  mutate(journal = case_when(
    str_detect(journal, "wiener tierärztliche monatsschrift") ~ "wiener tierarztliche monatsschrift",
    str_detect(journal, "applied animal welfare science jaaws")  ~ "applied animal welfare science",
      str_detect(journal, "javma journal of the american veterinary medical association")  ~ "journal of the american veterinary medical association",
    str_detect(journal, "animals an open access journal from mdpi")  ~ "animals",
    str_detect(journal, "anthrozoo s")  ~ "anthrozoos",
    TRUE ~ journal
    )
  )


# view the data
tidier2 %>%
  count(journal, sort=TRUE) %>%
  arrange(desc(journal))

```

Let's make a count of articles and see what percent of the total number if papers we have. 

```{r}
# make a new dataframe made of journals and counts by journal
mostpublished <- tidier2 %>%
  count(journal, sort=TRUE) %>%
  arrange(desc(journal))

# now add a column that is ia percentage of 312 so we can see what percent of our data is from which journal
mostpublished <- mostpublished %>% 
  group_by(journal) %>% 
  mutate(Percentage=paste0(round(n/312*100,2),"%"))

```

The top 5 out of 78 journals cover 148 (47.44%) articles, thus containing less than half the dataset. Only two journals contained more than 10% (>31 articles) of the data. The first dog-specific journal contained 7 (2.24%) articles and the first welfare-specific journal contained 5 (1.6%) of articles. All articles with >1% of the data were English / non-translated Journals.

Print a list of the top ten Journals.
```{r}
top10pub <- mostpublished %>% 
  rename("Count" = "n", "Journal" = "journal") %>% 
  arrange(desc(Count)) %>% 
  head(10)
print(top10pub)
write.csv(top10pub,"Top10Publishers.csv")
```


Let's view on a histogram. 
```{r echo=FALSE}
ggplot(tidier2, aes(journal)) + 
  geom_bar(color = "burlywood4", fill = "darkslategray") +
  theme(axis.text.y = element_text(size = 8)) +
  geom_text(stat='count', aes(label=..count..), vjust=-1) +
  labs(
    title = "Barplot of Article Publication by Journal",
    subtitle = "From ND to May 2022",
    caption = "Data of articles included after title and abstract screening.",
    x = "Journal publisher",
    y = "Count of articles published"
  ) +
  coord_flip() +
  theme_classic()
```



I've made these and I'll get back to them once I clean my data

```{r}
ggplot(tidier2, aes(x = year, y = journal, fill=journal)) +
  geom_density_ridges(stat="binline", bins=25, scale=.7) +
  theme_ridges()+ 
  theme(legend.position = "none", axis.text.y = element_text(size = 8)) +
  scale_x_continuous(limits=c(1965, 2025), breaks=seq(1965,2025,5)) +
  labs(
    title = "Ridgeline Histogram of Journal Publication over Time",
    subtitle = "From ND to May 2022",
    caption = "Data of articles included after title and abstract screening. Plots are colored by journal.",
    x = "Year of Publication",
    y = "Title of Journal"
  )

ggsave("RidgelinePublish.pdf", width = 12, height = 10)
```

From the histograms, it looks like there's a boom around 2000. Let's focus on these years.

```{r}
ggplot(tidier2, aes(x = year, y = journal, fill=journal)) +
  geom_density_ridges(stat="binline", bins=25, scale=0.8) +
  theme_ridges()+ 
  theme(legend.position = "none", axis.text.y = element_text(size = 8)) +
  scale_x_continuous(limits=c(2002, 2022), breaks=seq(2002,2022,5)) +
  labs(
    title = "Ridgeline Histogram of Journal Publication over Time",
    subtitle = "From 2002 to May 2022",
    caption = "Data of articles included after title and abstract screening and coloured by journal. 23 journals contained publications prior to 2002 not shown in this visualisation. Journals with no line only published articles prior to 2002.",
    x = "Year of Publication",
    y = "Title of Journal"
  )
ggsave("RidgelinePublish2022.pdf", width = 12, height = 10)
```


# Publication by Country
Now that we've got some idea of what we're looking at, let's look at publications by country and date. To do this we must parse out the countries from the strings. We'll use countrycode to do this.

We need the code to read through the string, pull out complete countries and make them as a list, then to select the first country to correspond to the first author. 

That said, a few things needed cleaning first. That includes removing punctuation, putting to lowercase, and correcting the NAs that appeared when first running the data.



```{r}
library(tidyr)
library(dplyr)
#install.packages("countrycode")
library(countrycode)
all_country <- countryname_dict %>% 
  # filter out non-ASCII country names:
  filter(grepl('[A-Za-z]', country.name.alt)) %>%
  # define column `country.name.alt` as an atomic vector:
  pull(country.name.alt) %>% 
  # change to lower-case:
  tolower()

# define alternation pattern of all country names:
library(stringr)
pattern <- str_c(all_country, collapse = '|')  # A huge alternation pattern!

df %>%
  # extract country name matches
  mutate(country = str_extract_all(tolower(text), pattern)) %>%
  unnest(country, keep_empty = TRUE)

```
Pull the country name from a list of countries, then alter the location data by removing punctuation and trying to correct some name variations that aren't being identified or are otherwise being problematic.
```{r}

#add a function
all_country <- countrycode::countryname_dict %>%  # remember the :: specifies the function specific to that package
                  filter(grepl('[A-Za-z]', country.name.en)) %>% # filter out non-ASCII country names:
                  pull(country.name.en) %>%  # define column `country.name.alt` as an atomic vector
                  tolower()
pattern <- str_c(all_country, collapse = '|')


# lets make a new dataset to work with
locationdata <- tidier2
locationdata$locations = locationdata$location

# remove punctuation
# locationdata$location = gsub("[[:punct:]\n]","", locationdata$location)

locationdata$locations %<>%
  gsub("[[:punct:]\n]","", .) %>%
  gsub("USA", "United States", .) %>%
  gsub("UK", "United Kingdom", .) %>%  
  gsub("England", "United Kingdom", .) %>%
  gsub("Korea", "South Korea", .) %>%
  gsub("Parma", "Italy", .) %>%
  gsub("New Brunswick NJ", "United States", .) %>%
  gsub("Saint Kitts and Nevis", "Saint Kitts and Nevis", .) %>%
  gsub("Berlin", "Germany", .)

```
Now we make a new table that is expanded so that every country found in the string has its own row, such that two authors = two rows
```{r}
locationtwo <- locationdata %>%
  mutate(country = str_extract_all(tolower(locationdata$locations), pattern)) %>%
  # select(-location) %>%
  unnest(country, keep_empty = TRUE)
```

Now to get only the first author's location.
```{r}
falocationdata <-
locationtwo %>% 
  group_by(key) %>% 
  filter(row_number()==1) %>%
  mutate(country = str_to_title(country))

```

After manual inspection of the entire dataset, some codes were still not correct to the first author affiliated country. Here is a manual correction of the affected country codes.
```{r}
falocationdata$country[falocationdata$key =="rayyan-825284849"] <- "United States"
falocationdata$country[falocationdata$key =="rayyan-825284950"] <- "Saint Kitts and Nevis"
falocationdata$country[falocationdata$key =="rayyan-825285156"] <- "Germany"
falocationdata$country[falocationdata$key =="rayyan-825285209"] <- "Italy"
falocationdata$country[falocationdata$key =="rayyan-875292441"] <- "United States"
falocationdata$country[falocationdata$key =="rayyan-875295819"] <- "Italy"
falocationdata$country[falocationdata$key =="rayyan-875297343"] <- "Germany"
falocationdata$country[falocationdata$key =="rayyan-875297370"] <- "Austria"

```

  
  
  Finally we can plot the number or aricles by country.
```{r}

ggplot(falocationdata, aes(country)) + 
  geom_bar(color = "midnightblue", fill = "aquamarine4") +
  geom_text(stat='count', aes(label=..count..), vjust=-1) +
  labs(
    title = "Barplot of Article Publication by First Author Country",
    subtitle = "From ND to May 2022",
    caption = "Data of articles included after title and abstract screening.",
    x = "Country of First Author Affiliation or Correspondance",
    y = "Count of articles published"
  ) +
  theme_classic() + 
  theme(axis.text.x = element_text(angle = 45, hjust=1), plot.margin = margin(2,1,1,1, "cm"))

ggsave("PublicationbyCountry.pdf", width = 21, height = 20, units = "cm")
```
The top 5 countries produced 58% (n=181) of all included data.


Let's manually check if if first author information is correct from a subset...
```{r}
falocationdata %>%
  filter(str_detect(country, 'united kingdom'))

```

Now to look at country and journal publication both. So look at breakdown of publication within country.
I selected all of the entries labelled as Italy in the country and looked at the original location column. All the first author locatinos are correct. And Italy was even selected when the second authors were from a different country, suggesting it correctly selected the first author location. 

Then I did the same with UK, which is really important because it required some data cleaning and the numbers kept changing as I cleaned too.
There was 1 error. All the others were accurate even when second author was from another country. 
One of the data should have included Saint Kitts and Nevis, which did show up in an earlier uncorrected version. I'll see if I can fix this. 
Ross University School of veterinary Medicine, Basseterre, St. Kitts, Saint Kitts and Nevis
```{r}
agglocation <- falocationdata %>% group_by(country, journal) %>%
  count(journal, sort=TRUE)
  
```

It's good to tale a look at this but there are too many journals for good information to come of it.
```{r}
ggplot(agglocation, aes(fill=journal, y=n, x=country)) + 
    geom_bar(position="stack", stat="identity") +
    theme_classic() + 
    theme(legend.position="bottom", axis.text.x = element_text(angle = 45, hjust=1))
```

Now let's put this on a map!
We want count by country connected to countries with  coordinates then put over an image of a map with coordinates so we can make a visual.
This code is based off the guide by Dr Paul Christiansen available at https://youtu.be/AgWgPSZ7Gp0

```{r}
# Count of articles by country
aggcountry <- falocationdata %>% group_by(country) %>%
  count(country, sort=TRUE)

# country coordinates
mapdata <- map_data("world")

# Clean the dataset
mapdata$region[mapdata$region =="Saint Kitts" | mapdata$region =="Nevis"] <- "Saint Kitts and Nevis"
mapdata$region[mapdata$region =="USA"] <- "United States"
mapdata$region[mapdata$region =="UK"] <- "United Kingdom"

# Join the datasets
mapdata <- left_join(mapdata, aggcountry, by= c("region" = "country"))

```
Mapping points over the globe.
```{r}

globemap2 <- ggplot() + 
  geom_polygon(data=mapdata, aes(fill=n, x=long, y=lat, group=group)) +
  scale_fill_gradient(name="Count", low="aquamarine3", high="midnightblue", na.value="snow4") +
  labs(
    title = "Map of Included Articles by Country",
    subtitle = "Article count by country of first author or corresponding author.",
    # caption = "Grey indicates no publications while darker blue shades indicate a higher published count. Publishing country is determined by the first author listed country or corresponding address; whichever was pulled from the database data."
  ) +
  theme_void() +
  coord_fixed(1.2) +
  theme(plot.title = element_text(size = 12, face = "bold", family="Kohinoor Devanagari"),
    legend.title=element_text(size=10, family="Kohinoor Devanagari"), 
    legend.text=element_text(size=9, family="Kohinoor Devanagari"))
globemap2
  
ggsave("PublicationMap.png", width = 7, height = 4)
```
Try on with a grouped scale
```{r}

globemap2 <- ggplot() + 
  geom_polygon(data=mapdata, aes(fill=as.factor(n), x=long, y=lat, group=group)) +
  # geom_polygon(aes(fill = oil_bbl_per_day, color = as.factor(opec_ind))) +
#  scale_fill_gradient(name="Count of articles", low="aquamarine3", high="midnightblue", na.value="snow4") +
#  scale_fill_gradientn(colours = c('#461863','#404E88','#2A8A8C','#7FD157','#F9E53F'),values = scales::rescale(c(0,5,10,15,25, 30)) ,breaks = c(0,5,10,15,25, 30)) +
  labs(
    title = "Map of Included Articles by Country",
    subtitle = "Count of articles published by country of first author or corresponding author.",
    # caption = "Grey indicates no publications while darker blue shades indicate a higher published count. Publishing country is determined by the first author listed country or corresponding address; whichever was pulled from the database data."
  ) +
  theme_void() +
  coord_fixed(1.2) +
  theme(plot.title = element_text(size = 12, face = "bold"),
    legend.title=element_text(size=10), 
    legend.text=element_text(size=9))
globemap2
  
ggsave("PublicationMap.jpeg", width = 8, height = 4)
```

Tutorials for maps are here:
https://www.happykhan.com/posts/map-projections-in-r/
https://bookdown.org/content/b298e479-b1ab-49fa-b83d-a57c2b034d49/map.html#basic-map
https://bookdown.org/content/b298e479-b1ab-49fa-b83d-a57c2b034d49/map.html#plot-it-with-ggplot2


#Text Mining and Word Relationships
As a reminder: 
Text mining was used to pull relevant words from abstracts to explore:
- frequency of word
- word frequency by year
- with correlation tests 
Sentiment analysis was used to 
- separate words into sentiments, such as words preceded by 'not' belonging to negative sentiments, and 
- word count by sentiment. 
Analysis included 
- visualising a network of words used together and a correlation of words including 
- a visualisation of correlations within word clusters. 

Start here: https://afit-r.github.io/tidy_text
End here: https://afit-r.github.io/word_relationships

turn the dataframe to tibble

As we will see below, there is a problem with combining words. Even when stemming words, the UK vs  US spelling of behaviour becomes a problem, as the stems are still different. So I am also going to replace the incidence of behav with behaviour. It won't really make sense in all contexts, but since we're going to stem the words anyways this is needed now.

First make a document with the content we want to look at and make behaviour a word.

```{r}
abstracts <- tidier2 %>% select(key, title, abstract, year)


abstracts$abstract <- gsub("behav[^ ]*","behaviour",abstracts$abstract, ignore.case=TRUE)
```


#################### TEST ARENA. #################
```{r}
tidy_words <- abstracts %>% 
  unnest_tokens(output=word, input=abstract) %>%  # each word in its own row
  anti_join(stop_words, by='word') %>%  # anti join gets rid of any match of things to the stop words list
  filter(str_detect(word, "[:alpha:]"))  # makes sure words are used (alpha for alphabetical)
  
tidy_stems <- tidy_words %>%
  mutate(stem = wordStem(word, language = "en")) # mutate the data to replace the word with the stem of the word

# write_csv(tidy_stems, path = "RayyanExport06022023/tidy_stem_words.csv")
```

Let's look at word frequency, being careful to count the number of articles that contain a word, rather than number of times the word shows up.
```{r}
tidy_stems %>%
        distinct() %>% # we want the word to count once per article
        count(stem, sort = TRUE) %>%
        top_n(10) # just show top 10
```

Now lets look at word frequency by year

```{r}
tidy_stems %>%
    group_by(year) %>%
    # distinct() %>% # we want the word to count once per article
    # filter(year == 2021) %>%
    filter(between(year, 2011, 2021)) %>% 
    count(stem, sort = TRUE) %>%
    select(stem, n) %>%
    # head(.,10) %>%
    top_n(9) %>%
    # ungroup() %>%
    ggplot(aes(x = reorder(stem, year), y = n, fill=year)) +
    facet_wrap(~ year, ncol = 2, scales = "free") +
    geom_col() +
    labs(title="Words ",
         x = NULL,
         y = "Frequency") +
    coord_flip()
```
```{r}
tidy_stems %>%
  group_by(year) %>%
  filter(between(year, 2017, 2021)) %>% #graphs are easier to read in 6 year intervals, change this part to reflect different years#
  count(stem, sort = TRUE) %>%
  select(stem, n) %>%
  top_n(10, n) %>%
  slice(1:10) %>%
  ggplot(mapping = aes(x=reorder(stem, n), y = n, fill=year)) + #, fill=year
  geom_col() +
  #scale_fill_gradient2(low="aquamarine3", high="midnightblue") +
  # geom_col(scale_fill_gradient2(low="aquamarine3", high="midnightblue")) +
  facet_grid(~ year) +
  labs(title="Top Words by Publication Year",
       x = "Stem Word",
       y = "Frequency") +
  theme(text = element_text(size = 10, family="Kohinoor Devanagari"), legend.position="none") +
  scale_fill_gradient(low="aquamarine3", high="midnightblue") +
  coord_flip()

# ggsave("WordCountYear.png", width = 7, height = 5)
```



Doing a pairwise count for words that are grouped together (occur in the same article).
```{r}

word_cooccurences <- pairwise_count(tidy_stems, stem, key, sort = TRUE)

```

Filter for words that occur together at least 90 times.
```{r}
word_cooccurences %>%
        filter(n >= 90) %>%
        graph_from_data_frame() %>%
        ggraph(layout = "stress", circular = TRUE) +
        geom_edge_link(aes(edge_alpha = n, edge_width = n), color="snow4") +
        geom_node_point(color = "aquamarine3", size = 5) +
        geom_node_text(aes(label = name), vjust = 1.8,family="Kohinoor Devanagari") +
      labs(title="Stem Word Relations by Article",
         x = NULL,
         y = "Frequency") +
        theme_void()

ggsave("Network_article.png", width = 7, height = 5)
```
```{r}
tidy_stems %>%
  group_by(stem) %>%
  filter(n() >= 20) %>%
  pairwise_cor(stem, key) %>%
  filter(!is.na(correlation),
         correlation > .5) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "aquamarine3", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, family="Kohinoor Devanagari") +
  theme_void()

ggsave("Network_Words.png", width = 5.3, height = 3)
```


################### END TEST ARENA ###############


Word counts by unique articles. So how many articles use X word. I did filter out words that were only used once.
```{r}
abstracts_words <- abstracts %>% 
  unnest_tokens(output=word, input=abstract) %>%  # each word in its own row
  anti_join(stop_words, by='word') %>%  # anti join gets rid of any match of things to the stop words list
  filter(str_detect(word, "[:alpha:]")) %>%  # makes sure words are used (alpha for alphabetical)
  # distinct() # if same article lists word many times, only counts as one
```

### Here is code for looking at words without reducing words to their stem meanings. 
```{r}
articles_mention_words <- abstracts_words %>% 
  count(word, name='ArticleCount') %>% # how many articles mention X word
  filter(ArticleCount > 1) # words mentioned more than once %>% 
# so 31 is ten percent of the papers

# write.csv(articles_mention_words,"ArticleMentionWords_MoreThanOnce.csv")

toparticles_mention_words <- abstracts_words %>% 
  count(word, name='ArticleCount') %>% # how many articles mention X word
  filter(ArticleCount > 78) # words mentioned more than 10 percent of papers Change filter to 31 for top 90 percent, 78 for 75, 156 for 50
```

### New Step: Standardise Words 
Some words are the same but different tense, plural, etc. So I will need to standardise the words to compare them effectively. We'll use Snowball stemming as it has grown from contributions and so has a large vocabulary.


```{r}
abstracts_stems <- abstracts_words %>%
  mutate(stem = wordStem(word))

toparticles_wordstems <- abstracts_stems %>% 
  count(stem, name='ArticleCount') %>% # how many articles mention X word
  filter(ArticleCount > 78) # words mentioned more than 10 percent of papers Change filter to 31 for top 90 percent, 78 for 75, 156 for 50

ggplot(toparticles_wordstems, aes(ArticleCount)) + 
  geom_histogram(bins=60,color = "aquamarine3", fill = "midnightblue") +
  labs(
    title = "Unique Stem Words Among Articles",
    subtitle = "How similar are words used across articles?",
    # caption = "Caption if needed",
    x = "Number of Articles that Use the Same Word",
    y = "Count of Unique Stem Words"
  )
ggsave("UniqueStemWordCount.jpeg", width = 6, height = 4)
```

### Wordcloud

```{r}
stemcloud <- abstracts_stems %>% 
  count(stem, name='freq', sort=TRUE) # how many articles mention X word

png(filename = "plot3.png", width = 6, height = 4, units = "in", res = 300)
wordcloud(words = stemcloud$stem, freq = stemcloud$freq, min.freq = 5, max.words=100, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"), family="Kohinoor Devanagari")

# ggplot2::ggsave("wordclouddog.png", width = 5, height = 5)
```



### Unique words over time
I didn't save year with this information!



### Let's do a word correlation
Spearman's because of the skew of the data.

Wait, I did this analysis on the article count not the article name? How am I seeing which words correlate?


```{r}
# pairwise correlation (pearson correlation is default) use  method="spearman" if doing spearman

abs_word_relate <- toparticles_wordstems %>% 
  semi_join(toparticles_wordstems, by='stem') %>% 
  pairwise_cor(item=stem, feature = ArticleCount, method="spearman")
```




#### Visualisation of Correlations

Let's try a circulr correlation jsut to see what we're dealing with here.
```{r}

graph_from_data_frame(d=abs_word_relate, 
                      vertices = toparticles_wordstems %>% 
                        semi_join(abs_word_relate, by= c("stem" = "item1"))) %>% # remove words not connected
  
  
  ggraph(layout = 'stress', circular = TRUE) + 
  geom_edge_arc(aes(color = correlation)) +
  scale_edge_width(range = c(0.2, 2)) + 
  scale_color_manual(values=c("aquamarine3","midnightblue")) +
  geom_node_label(aes(label=name), 
              color = "midnightblue",          # text
              fill = "aquamarine3",         # label background
              size = 3,                 # font size
              label.r = unit(2, "pt"), # corner radius of label box
              label.size = .1,          # label border size
              label.padding = unit(.2, "lines")) +
#  geom_node_text(aes(label=name), size = 2) +
  theme_graph()
```
1. Difference colours or transparencies so we can see the relationships
2. or only display higer correlations

```{r}

col <- c("aquamarine", "aquamarine3", "turquoise4", "darkblue", "midnightblue")

graph_from_data_frame(d=abs_word_relate, 
                      vertices = toparticles_wordstems %>% 
                        semi_join(abs_word_relate, by= c("stem" = "item1"))) %>% # remove words not connected
  
  
  ggraph(layout="stress") +
  # geom_edge_link(aes(alpha = correlation), colour= "turquoise3") +
  geom_edge_link(aes(color = correlation)) +
  scale_edge_colour_gradient(low = "grey", high = "darkblue") +
  # geom_edge_link(aes(alpha = correlation), color = "midnightblue") +
  scale_edge_width(range = c(0.2, 2)) + 
  geom_node_label(aes(label=name), repel=T,
                color = "midnightblue",          # text
                fill = "aquamarine3",         # label background
                size = 2,                 # font size
                label.r = unit(2, "pt"), # corner radius of label box
                label.size = .1,          # label border size
                label.padding = unit(.2, "lines")) +
  #geom_node_text(aes(label=name), size = 4, color="black", repel=T) +
  labs(
  title = "Correlation between Words",
  subtitle = "Relationship of top 75% most used words",
  ) +
  theme_graph()

# ggsave("CorrelationWords.jpeg", width = 8, height = 6)

# Get a table of correlations to say specifically which were the strongest.

```

```{r}
graph_from_data_frame(d=abs_word_relate, 
                      vertices = toparticles_wordstems %>% 
                        semi_join(abs_word_relate, by= c("stem" = "item1"))) %>% # remove words not connected

ggraph(layout = "drl") + 
  geom_edge_link() + 
  geom_node_point()
```

And see what the top three most used methods are. Most used is not frequency overall but articles that have included the word.
This is very broad and shallow - word might be used in reference but not an actual method used. (will need to check that)
We will use the articles pulled and see which ones refer to a term but don't use it and can lightly extrapolate from that
Do sentiment analysis
plot sentiments

Top 3 Methods

End result: behaviour tops the chart here.

